#!/usr/bin/env python
# coding=utf-8

import rospy
import os
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
import time
import sys

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray

import random
from collections import deque

import numpy as np
import tensorflow as tf

from keras.layers import Input, Dense, Lambda, concatenate,normalization
from keras.models import Model
import keras.backend as K
from keras.models import Sequential, load_model
from keras.optimizers import RMSprop,Adam
from keras.layers import Dense, Dropout, Activation,merge
# 导入 Env
from src.turtlebot3_dqn.environment_stage_5 import Env

class DDPG():
    """Deep Deterministic Policy Gradient Algorithms.
    """
    def __init__(self, state_size, action_size):

        self.sess = K.get_session()
        self.bound = 1.5

        # update rate for target model.
        self.TAU = 0.001
        # experience replay.
        self.memory_buffer = deque(maxlen=1000000)
        # discount rate for q value.
        self.gamma = 0.95
        # epsilon of action selection
        self.epsilon = 1.0
        # discount rate for epsilon.
        self.epsilon_decay = 0.995
        # min epsilon of ε-greedy.
        self.epsilon_min = 0.01
        # 状态数
        self.state_size = state_size
        # 动作数
        self.action_size = action_size
        # actor learning rate
        self.a_lr = 0.0001
        # critic learining rate
        self.c_lr = 0.001
        # ddpg model
        self.critic = self._build_critic()
        self.actor = self._build_actor()

        self.env = Env(action_size)
        # target model
        self.target_actor = self._build_actor()
        self.target_actor.set_weights(self.actor.get_weights())
        self.target_critic = self._build_critic()
        self.target_critic.set_weights(self.critic.get_weights())

        # gradient function

        self.get_critic_grad = self.critic_gradient()
        self.actor_optimizer()

        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        self.dirPath = self.dirPath.replace('turtlebot3_dqn/nodes', 'turtlebot3_dqn/')
        #
        # if os.path.exists('model/ddpg_actor.h5') and os.path.exists('model/ddpg_critic.h5'):
        #     self.actor.load_weights('model/ddpg_actor.h5')
        #     self.critic.load_weights('model/ddpg_critic.h5')

    def _build_actor(self):
        """Actor model.
        """
        inputs = Input(shape=(self.state_size,), name='state_input')
        #normalization.BatchNormalization(gamma_initializer="one", epsilon=1e-06, beta_initializer="zero", weights=None,momentum=0.9, axis=-1)
        x = Dense(300, activation='relu')(inputs)
        x = Dense(200, activation='relu')(x)
        x = Dense(1, activation='tanh')(x)
        output = Lambda(lambda x: x * self.bound)(x)
        model = Model(inputs=inputs, outputs=output)
        #a_loss = self.critic.output
        #model.compile(loss=[-a_loss], optimizer=RMSprop(lr=self.a_lr, rho=0.9, epsilon=1e-06))
        model.compile(loss='mse', optimizer=Adam(lr=self.a_lr))

        return model

    def _build_critic(self):
        """Critic model.
        """
        dropout = 0.2
        sinput = Input(shape=(self.state_size,), name='state_input')
        ainput = Input(shape=(1,), name='action_input')
        #normalization.BatchNormalization(gamma_initializer="one",epsilon=1e-06, beta_initializer="zero", weights=None, momentum=0.9, axis=-1)
        s = Dense(300, activation='relu')(sinput)
        s = Dense(200, activation='linear')(s)
        a = Dense(200, activation='linear')(ainput)
        #x = concatenate([s, a])
        x = merge([s, a], mode='sum')
        x = Dense(200, activation='relu')(x)
        output = Dense(1, activation='linear')(x)
        model = Model(inputs=[sinput, ainput], outputs=output)
        model.compile(loss='mse', optimizer=Adam(lr=self.a_lr))

        return model

    def actor_optimizer(self):
        """actor_optimizer.

        Returns:
            function, opt function for actor.
        """
        self.ainput = self.actor.input
        aoutput = self.actor.output
        trainable_weights = self.actor.trainable_weights
        self.action_gradient = tf.placeholder(tf.float32, shape=(None, self.action_size))
        #tf.placeholder()函数作为一种占位符用于定义过程，可以理解为形参，在执行的时候再赋具体的值。

        # tf.gradients will calculate dy/dx with a initial gradients for y
        # action_gradient is dq / da, so this is dq/da * da/dparams
        params_grad = tf.gradients(aoutput, trainable_weights, -self.action_gradient)
        grads = zip(params_grad, trainable_weights)
        self.opt = tf.train.AdamOptimizer(self.a_lr).apply_gradients(grads)
        self.sess.run(tf.global_variables_initializer())

    def critic_gradient(self):
        """get critic gradient function.

        Returns:
            function, gradient function for critic.
        """
        cinput = self.critic.input
        coutput = self.critic.output

        # compute the gradient of the action with q value, dq/da.
        action_grads = K.gradients(coutput, cinput[1])

        return K.function([cinput[0], cinput[1]], action_grads)

    def OU(self, x, mu=0, theta=0.6, sigma=0.3):
        """Ornstein-Uhlenbeck process.
        formula：ou = θ * (μ - x) + σ * w

        Arguments:
            x: action value.
            mu: μ, mean fo values.
            theta: θ, rate the variable reverts towards to the mean.
            sigma：σ, degree of volatility of the process.

        Returns:
            OU value
        """
        return theta * (mu - x) + sigma * np.random.randn(1)

    def get_action(self, X):
        """get actor action with ou noise.

        Arguments:
            X: state value.
        """
        action = self.actor.predict(X)[0][0]
        print("action_no_OU:%.2f"%action)
        # add randomness to action selection for exploration
        noise = max(self.epsilon, 0) * self.OU(action)
        # np.clip用法，其中a是一个数组，后面两个参数分别表示最小和最大值，
        # 将数组中的元素限制在a_min, a_max之间，大于a_max的就使得它等于 a_max，小于a_min,的就使得它等于a_min
        #action = round(np.clip(action + noise, -self.bound, self.bound), 2)
        print("choose action%.2f"%action)
        return action

    def remember(self, state, action, reward, next_state, done):
        """add data to experience replay.

        Arguments:
            state: observation.
            action: action.
            reward: reward.
            next_state: next_observation.
            done: if game done.
        """
        item = (state, action, reward, next_state, done)
        self.memory_buffer.append(item)

    def update_epsilon(self):
        """update epsilon.
        """
        if self.epsilon >= self.epsilon_min:
            self.epsilon *= self.epsilon_decay


    def process_batch(self, batch):
        """process batch data.

        Arguments:
            batch: batch size.

        Returns:
            states: states.
            actions: actions.
            y: Q_value.
        """
        y = []
         # random choice batch data from experience replay.
        data = random.sample(self.memory_buffer, batch) #多用于截取列表的指定长度的随机数，但是不会改变列表本身的排序
        states = np.array([d[0] for d in data])
        actions = np.array([d[1] for d in data])
        next_states = np.array([d[3] for d in data])

        next_actions = self.target_actor.predict(next_states)
        # Q_target
        q = self.target_critic.predict([next_states, next_actions])
        #print("Q_taget:")
        #print(q)
        # update Q value
        for i, (_, _, reward, _, done) in enumerate(data):
            target = reward
            if not done:
                target += self.gamma * q[i][0]
            y.append(target)
        #print("y:")
        #print(y)
        return states, actions, y

    def update_model(self, X1, X2, y):
        """update ddpg model.

        Arguments:
            states: states.
            actions: actions.
            y: Q_value.

        Returns:
            loss: critic loss.
        """
        loss = self.critic.train_on_batch([X1, X2], y)
        #loss = self.critic.fit([X1, X2], y, verbose=0)
        #loss = np.mean(loss.history['loss'])

        X3 = self.actor.predict(X1)
        a_grads = np.array(self.get_critic_grad([X1, X3]))[0]
        self.sess.run(self.opt, feed_dict={
            self.ainput: X1,
            self.action_gradient: a_grads
        })

        return loss

    def update_target_model(self):
        """soft update target model.
        formula：θ​​t ← τ * θ + (1−τ) * θt, τ << 1.
        """
        critic_weights = self.critic.get_weights()
        actor_weights = self.actor.get_weights()
        critic_target_weights = self.target_critic.get_weights()
        actor_target_weights = self.target_actor.get_weights()

        for i in range(len(critic_weights)):
            critic_target_weights[i] = self.TAU * critic_weights[i] + (1 - self.TAU) * critic_target_weights[i]

        for i in range(len(actor_weights)):
            actor_target_weights[i] = self.TAU * actor_weights[i] + (1 - self.TAU) * actor_target_weights[i]

        self.target_critic.set_weights(critic_target_weights)
        self.target_actor.set_weights(actor_target_weights)

    def play(self):
        """play game with model.
        """
        print('play...')
        observation = self.env.reset()

        reward_sum = 0
        random_episodes = 0

        while random_episodes < 10:

            x = observation.reshape(-1, 3)
            action = self.actor.predict(x)[0]
            observation, reward, done = self.env.step(action)

            reward_sum += reward

            if done:
                print("Reward for this episode was: {}".format(reward_sum))
                random_episodes += 1
                reward_sum = 0
                observation = self.env.reset()


if __name__ == '__main__':
    rospy.init_node('turtlebot3_dqn_stage_5')
    #pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    #pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    #result = Float32MultiArray()
    #get_action = Float32MultiArray()

    state_size = 9
    action_size = 1

    agent = DDPG(state_size, action_size)
    scores, episodes = [], []
    start_time = time.time()


    #model = DDPG(state_size, action_size)

    #history = model.train(200, 128)
    #model.save_history(history, 'ddpg.csv')

    #model.play()

    """training model.
            Arguments:
                episode: ganme episode.
                batch： batch size of episode.

            Returns:
                history: training history.
            """
    history = {'episode': [], 'Episode_reward': [], 'Loss': []}

    for e in range(3000):
        state = agent.env.reset()
        reward_sum = 0
        losses = []
        done = False

        for t in range(500):
            # chocie action from ε-greedy.
            print(state)
            x = state.reshape(1, state.shape[0])
            print(x)
            # actor action
            action = agent.get_action(x)
            #print("action5:")
            #print(action)
            next_state, reward, done = agent.env.step(action)
            # add data to experience replay.
            agent.remember(x[0], action, reward, next_state, done)

            if len(agent.memory_buffer) > 128:
                X1, X2, y = agent.process_batch(64)

                # update DDPG model
                loss = agent.update_model(X1, X2, y)
                # update target model
                agent.update_target_model()
                # reduce epsilon pure batch.
                agent.update_epsilon()

                losses.append(loss)
            reward_sum += reward
            state = next_state
            #print("time_t:")
            #print(t)
            # 超过200步时设定为超时，回合结束
            if t >= 200:
                rospy.loginfo("Time out!!")
                reward_sum -= 200
                done = True
            if done:
                # 发布result话题
                # result.data = [reward_sum, np.max(agent.q_value)]
                # pub_result.publish(result)
                scores.append(reward_sum)
                episodes.append(e)
                # 计算运行时间
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)

                rospy.loginfo('Ep: %d score: %.2f memory: %d epsilon: %.2f time: %d:%02d:%02d',
                              e, reward_sum, len(agent.memory_buffer), agent.epsilon, h, m, s)
                break

            #step += 1
        loss = np.mean(losses)
        history['episode'].append(e)
        history['Episode_reward'].append(reward_sum)
        history['Loss'].append(loss)
        print('Episode: {}/{} | reward: {} | loss: {:.3f}'.format(e, 3000, reward_sum, loss))

        agent.actor.save_weights(agent.dirPath+'model/ddpg_actor.h5')
        agent.critic.save_weights(agent.dirPath+'model/ddpg_critic.h5')



        #打印距离，观察距离是否在目标点变化后有改变